{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataclasses'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6386cd771911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnb_003a\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/fastai_v1/dev_nb/nb_003a.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnb_002\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnb_002c\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/fastai_v1/dev_nb/nb_002.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnb_001b\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/fastai_v1/dev_nb/nb_001b.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataclasses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataclasses'"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from nb_003a import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data')\n",
    "PATH = DATA_PATH/'cifar10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean,data_std = map(tensor, ([0.491, 0.482, 0.447], [0.247, 0.243, 0.261]))\n",
    "cifar_norm = normalize_tfm(mean=data_mean,std=data_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [flip_lr_tfm(p=0.5),\n",
    "        pad_tfm(padding=4),\n",
    "        crop_tfm(size=32, row_pct=(0,1.), col_pct=(0,1.))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FilesDataset.from_folder(PATH/'train', classes=['airplane','dog'])\n",
    "valid_ds = FilesDataset.from_folder(PATH/'test', classes=['airplane','dog'])\n",
    "data = DataBunch.create(train_ds, valid_ds, bs=bs, train_tfm=tfms, valid_tfm=[], num_workers=4)\n",
    "len(data.train_dl), len(data.valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, xb, yb, loss_fn, opt=None):\n",
    "    loss = loss_fn(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl):\n",
    "    for epoch in tnrange(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            loss,_ = loss_batch(model, xb, yb, loss_fn, opt)\n",
    "            if train_dl.progress_func is not None: train_dl.gen.set_postfix_str(loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses,nums = zip(*[loss_batch(model, xb, yb, loss_fn)\n",
    "                                for xb,yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, data, model):\n",
    "        self.data,self.model = data,model.to(data.device)\n",
    "\n",
    "    def fit(self, epochs, lr, opt_fn=optim.SGD):\n",
    "        opt = opt_fn(self.model.parameters(), lr=lr)\n",
    "        loss_fn = F.cross_entropy\n",
    "        fit(epochs, self.model, loss_fn, opt, self.data.train_dl, self.data.valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(ni, nf, ks=3, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ni, nf, kernel_size=ks, bias=False, stride=stride, padding=ks//2),\n",
    "        nn.BatchNorm2d(nf),\n",
    "        nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "\n",
    "class ResLayer(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.conv1=conv_layer(ni, ni//2, ks=1)\n",
    "        self.conv2=conv_layer(ni//2, ni, ks=3)\n",
    "        \n",
    "    def forward(self, x): return x + self.conv2(self.conv1(x))\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    def make_group_layer(self, ch_in, num_blocks, stride=1):\n",
    "        return [conv_layer(ch_in, ch_in*2,stride=stride)\n",
    "               ] + [(ResLayer(ch_in*2)) for i in range(num_blocks)]\n",
    "\n",
    "    def __init__(self, num_blocks, num_classes, nf=32):\n",
    "        super().__init__()\n",
    "        layers = [conv_layer(3, nf, ks=3, stride=1)]\n",
    "        for i,nb in enumerate(num_blocks):\n",
    "            layers += self.make_group_layer(nf, nb, stride=2-(i==1))\n",
    "            nf *= 2\n",
    "        layers += [nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(nf, num_classes)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x): return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 4, 6, 3], num_classes=10, nf=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters easily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want an optimizer with an easy way to set hyperparameters: they're all properties and we define custom setters to handle the different names in pytorch optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HPOptimizer():\n",
    "    \n",
    "    def __init__(self, params, opt_fn, init_lr, true_wd=False):\n",
    "        self.opt = opt_fn(params, init_lr)\n",
    "        self._lr, self.true_wd = init_lr, true_wd\n",
    "        self.opt_keys = list(self.opt.param_groups[0].keys())\n",
    "        self.opt_keys.remove('params')\n",
    "        self.read_defaults()\n",
    "    \n",
    "    #Pytorch optimizer methods\n",
    "    def step(self):\n",
    "        #Does the weight decay outside of the optimizer step for AdamW\n",
    "        if self.true_wd:\n",
    "            for pg in self.opt.param_groups:\n",
    "                for p in pg['params']:\n",
    "                    p.data.mul_(1 - self._wd * pg['lr'])\n",
    "            self.set_val('weight_decay', 0)\n",
    "        self.opt.step()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad()\n",
    "    \n",
    "    #Hyperparameters as properties\n",
    "    @property\n",
    "    def lr(self): return self._lr\n",
    "\n",
    "    @lr.setter\n",
    "    def lr(self, val):\n",
    "        self.set_val('lr', val)\n",
    "        self._lr = val\n",
    "    \n",
    "    @property\n",
    "    def mom(self): return self._mom\n",
    "\n",
    "    @mom.setter\n",
    "    def mom(self, val):\n",
    "        if 'momentum' in self.opt_keys: self.set_val('momentum', val)\n",
    "        elif 'betas' in self.opt_keys:  self.set_val('betas', (val, self._beta))\n",
    "        self._mom = val\n",
    "    \n",
    "    @property\n",
    "    def beta(self): return self._beta\n",
    "\n",
    "    @beta.setter\n",
    "    def beta(self, val):\n",
    "        if 'betas' in self.opt_keys:    self.set_val('betas', (self._mom,val))\n",
    "        elif 'alpha' in self.opt_keys:  self.set_val('alpha', val)\n",
    "        self._beta = val\n",
    "    \n",
    "    @property\n",
    "    def wd(self): return self._wd\n",
    "\n",
    "    @wd.setter\n",
    "    def wd(self, val):\n",
    "        if not self.true_wd: self.set_val('weight_decay', val)\n",
    "        self._wd = val\n",
    "    \n",
    "    #Helper functions\n",
    "    def read_defaults(self):\n",
    "        if 'momentum' in self.opt_keys: self._mom = self.opt.param_groups[0]['momentum']\n",
    "        if 'alpha' in self.opt_keys: self._beta = self.opt.param_groups[0]['alpha']\n",
    "        if 'betas' in self.opt_keys: self._mom,self._beta = self.opt.param_groups[0]['betas']\n",
    "        if 'weight_decay' in self.opt_keys: self._wd = self.opt.param_groups[0]['weight_decay']\n",
    "    \n",
    "    def set_val(self, key, val):\n",
    "        for pg in self.opt.param_groups: pg[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = HPOptimizer(model.parameters(), opt_fn, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.lr, opt.mom, opt.wd, opt.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.lr=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.lr, opt.mom, opt.wd, opt.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's easy to set and change the HP in the optimizer, we need a scheduler to change it. To keep the training loop as readable as possible we don't want to handle all of this stuff inside it so we'll use callbacks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback():\n",
    "    def on_train_begin(self): pass         \n",
    "        #To initiliaze constants in the callback.\n",
    "    def on_epoch_begin(self): pass\n",
    "        #At the beginning of each epoch\n",
    "    def on_batch_begin(self, xb, yb): pass \n",
    "        #To set HP before the step is done. A look at the input can be useful (set the lr depending on the seq_len in RNNs, \n",
    "        #or for reg_functions called in on_backward_begin)\n",
    "        #Returns xb, yb (which can allow us to modify the input at that step if needed)\n",
    "    def on_backward_begin(self, loss, out): pass\n",
    "        #Called after the forward pass and the loss has been computed, but before the back propagation.\n",
    "        #Passes the loss and the output of the model.\n",
    "        #Returns the loss (which can allow us to modify it, for instance for reg functions)\n",
    "    def on_backward_end(self): pass\n",
    "        #Called after the back propagation had been done (and the gradients computed) but before the step of the optimizer.\n",
    "        #Useful for true weight decay in AdamW\n",
    "    def on_step_end(self): pass\n",
    "        #Called after the step of the optimizer but before the gradients are zeroed (not sure this one is useful)\n",
    "    def on_batch_end(self, loss): pass\n",
    "        #Called at the end of the batch\n",
    "    def on_epoch_end(self, val_loss): pass\n",
    "        #Called at the end of an epoch\n",
    "    def on_train_end(self): pass\n",
    "        #Useful for cleaning up things and saving files/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to have a callback between every line of the training loop, that way everything we need to add will be treated there and not inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, xb, yb, loss_fn, opt=None, callbacks=[]):\n",
    "    out = model(xb)\n",
    "    loss = loss_fn(out, yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        for cb in callbacks: loss = cb.on_backward_begin(loss, out)\n",
    "        loss.backward()\n",
    "        for cb in callbacks: cb.on_backward_end()\n",
    "        opt.step()\n",
    "        for cb in callbacks: cb.on_step_end()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl,callbacks=[]):\n",
    "    for cb in callbacks: cb.on_train_begin()\n",
    "    for epoch in tnrange(epochs):\n",
    "        model.train()\n",
    "        for cb in callbacks: cb.on_epoch_begin()\n",
    "        for xb,yb in train_dl:\n",
    "            for cb in callbacks: xb, yb = cb.on_batch_begin(xb, yb)\n",
    "            loss,_ = loss_batch(model, xb, yb, loss_fn, opt)\n",
    "            if train_dl.progress_func is not None: train_dl.gen.set_postfix_str(loss)\n",
    "            for cb in callbacks: cb.on_batch_end(loss)\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses,nums = zip(*[loss_batch(model, xb, yb, loss_fn)\n",
    "                                for xb,yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "        \n",
    "        for cb in callbacks: cb.on_epoch_end(val_loss)\n",
    "        print(epoch, val_loss)\n",
    "    for cb in callbacks: cb.on_train_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First callback: updating the progress bar can be done and printing the validation loss in one. We'll also keep track of the losses and hyper-parameters during training for future plots (lr_finder, plot of the LR/mom schedule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recorder(Callback):\n",
    "    \n",
    "    def __init__(self, opt, train_dl=None):\n",
    "        self.opt,self.train_dl = opt,train_dl\n",
    "    \n",
    "    def on_train_begin(self):\n",
    "        self.epoch = 0\n",
    "        self.losses,self.val_losses,self.lrs,self.moms = [],[],[],[]\n",
    "    \n",
    "    def on_batch_begin(self, xb, yb):\n",
    "        self.lrs.append(self.opt.lr)\n",
    "        self.moms.append(self.opt.mom)\n",
    "        return xb, yb\n",
    "    \n",
    "    def on_batch_end(self, loss):\n",
    "        self.losses.append(loss)\n",
    "        if self.train_dl is not None and self.train_dl.progress_func is not None: \n",
    "            self.train_dl.gen.set_postfix_str(loss)\n",
    "    \n",
    "    def on_epoch_end(self, val_loss):\n",
    "        self.val_losses.append(val_loss)\n",
    "        print(self.epoch, val_loss)\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl=None,callbacks=[]):\n",
    "    for cb in callbacks: cb.on_train_begin()\n",
    "    for epoch in tnrange(epochs):\n",
    "        model.train()\n",
    "        for cb in callbacks: cb.on_epoch_begin()\n",
    "        for xb,yb in train_dl:\n",
    "            for cb in callbacks: xb, yb = cb.on_batch_begin(xb, yb)\n",
    "            loss,_ = loss_batch(model, xb, yb, loss_fn, opt)\n",
    "            for cb in callbacks: cb.on_batch_end(loss)\n",
    "        \n",
    "        if valid_dl is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                losses,nums = zip(*[loss_batch(model, xb, yb, loss_fn)\n",
    "                                for xb,yb in valid_dl])\n",
    "            val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "        else: val_loss = None\n",
    "        for cb in callbacks: cb.on_epoch_end(val_loss)\n",
    "        \n",
    "    for cb in callbacks: cb.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, data, model):\n",
    "        self.data,self.model = data,model.to(data.device)\n",
    "        self.loss_fn, self.opt_fn = F.cross_entropy, optim.SGD\n",
    "\n",
    "    def fit(self, epochs, lr):\n",
    "        self.opt = HPOptimizer(self.model.parameters(), self.opt_fn, init_lr=lr)\n",
    "        self.recorder = Recorder(self.opt, self.data.train_dl)\n",
    "        fit(epochs, self.model, self.loss_fn, self.opt, self.data.train_dl, self.data.valid_dl, callbacks=[self.recorder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "learn = Learner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(2,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all very well but what if someone forgets to return xb,yb or the loss in the callbacks that can change it? To be more convenient and make the code of the training loop cleaner, we'll create a class to handle the callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback():\n",
    "    def on_train_begin(self): pass         \n",
    "        #To initiliaze constants in the callback.\n",
    "    def on_epoch_begin(self): pass\n",
    "        #At the beginning of each epoch\n",
    "    def on_batch_begin(self, xb, yb): pass \n",
    "        #To set HP before the step is done. A look at the input can be useful (set the lr depending on the seq_len in RNNs, \n",
    "        #or for reg_functions called in on_backward_begin)\n",
    "        #Returns xb, yb (which can allow us to modify the input at that step if needed)\n",
    "    def on_loss_begin(self, out): pass\n",
    "        #Called after the forward pass but before the loss has been computed.\n",
    "        #Passes the output of the model.\n",
    "        #Returns the output (which can allow us to modify it)\n",
    "    def on_backward_begin(self, loss): pass\n",
    "        #Called after the forward pass and the loss has been computed, but before the back propagation.\n",
    "        #Passes the loss of the model.\n",
    "        #Returns the loss (which can allow us to modify it, for instance for reg functions)\n",
    "    def on_backward_end(self): pass\n",
    "        #Called after the back propagation had been done (and the gradients computed) but before the step of the optimizer.\n",
    "        #Useful for true weight decay in AdamW\n",
    "    def on_step_end(self): pass\n",
    "        #Called after the step of the optimizer but before the gradients are zeroed (not sure this one is useful)\n",
    "    def on_batch_end(self, loss): pass\n",
    "        #Called at the end of the batch\n",
    "    def on_epoch_end(self, val_loss): pass\n",
    "        #Called at the end of an epoch\n",
    "    def on_train_end(self): pass\n",
    "        #Useful for cleaning up things and saving files/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallbackHandler():\n",
    "    \n",
    "    def __init__(self, callbacks):\n",
    "        self.callbacks = callbacks\n",
    "    \n",
    "    def __call__(self, cb_name, *args, **kwargs):\n",
    "        return [getattr(cb, f'on_{cb_name}')(*args, **kwargs) for cb in self.callbacks]\n",
    "    \n",
    "    def on_train_begin(self): self('train_begin')\n",
    "    def on_epoch_begin(self): self('epoch_begin')\n",
    "        \n",
    "    def on_batch_begin(self, xb, yb):\n",
    "        for cb in self.callbacks:\n",
    "            a = cb.on_batch_begin(xb,yb)\n",
    "            if a is not None: xb,yb = a\n",
    "        return xb,yb\n",
    "    \n",
    "    def on_loss_begin(self, out):\n",
    "        for cb in self.callbacks:\n",
    "            a = cb.on_loss_begin(out)\n",
    "            if a is not None: out = a\n",
    "        return out\n",
    "    \n",
    "    def on_backward_begin(self, loss):\n",
    "        for cb in self.callbacks:\n",
    "            a = cb.on_backward_begin(loss)\n",
    "            if a is not None: loss = a\n",
    "        return loss\n",
    "    \n",
    "    def on_backward_end(self):        self('backward_end')\n",
    "    def on_step_end(self):            self('step_end')\n",
    "    def on_batch_end(self, loss):     return np.any(self('batch_end', loss))\n",
    "    def on_epoch_end(self, val_loss): return np.any(self('epoch_end', val_loss))\n",
    "    def on_train_end(self):           self('train_end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, xb, yb, loss_fn, opt=None, cb_handler=CallbackHandler([])):\n",
    "    out = model(xb)\n",
    "    out = cb_handler.on_loss_begin(out)\n",
    "    loss = loss_fn(out, yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss = cb_handler.on_backward_begin(loss)\n",
    "        loss.backward()\n",
    "        cb_handler.on_backward_end()\n",
    "        opt.step()\n",
    "        cb_handler.on_step_end()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl=None, callbacks=[]):\n",
    "    cb_handler = CallbackHandler(callbacks)\n",
    "    cb_handler.on_train_begin()\n",
    "    \n",
    "    for epoch in tnrange(epochs):\n",
    "        model.train()\n",
    "        cb_handler.on_epoch_begin()\n",
    "        \n",
    "        for xb,yb in train_dl:\n",
    "            xb, yb = cb_handler.on_batch_begin(xb, yb)\n",
    "            loss,_ = loss_batch(model, xb, yb, loss_fn, opt, cb_handler)\n",
    "            if cb_handler.on_batch_end(loss): break\n",
    "        \n",
    "        if valid_dl is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                losses,nums = zip(*[loss_batch(model, xb, yb, loss_fn)\n",
    "                                for xb,yb in valid_dl])\n",
    "            val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "        else: val_loss=None\n",
    "        if cb_handler.on_epoch_end(val_loss): break\n",
    "        \n",
    "    cb_handler.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "learn = Learner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(2,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do a 1cycle scheduler pretty easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def annealing_no(start, end, pct): return start\n",
    "def annealing_linear(start, end, pct): return start + pct * (end-start)\n",
    "def annealing_exponential(start, end, pct): return start * (end/start) ** pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def is_tuple(x): return isinstance(x, tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Stepper():\n",
    "    \n",
    "    def __init__(self, vals, num_it, ft=None):\n",
    "        if is_tuple(vals): self.start,self.end = vals\n",
    "        else:              self.start,self.end = vals, 0\n",
    "        #Why doesn't this one work?\n",
    "        #(self.start,self.end) = (vals[0],vals[1]) if is_tuple(vals) else vals,0\n",
    "        self.num_it = num_it\n",
    "        if ft is None: self.ft = annealing_linear if is_tuple(vals) else annealing_no\n",
    "        else:          self.ft = ft\n",
    "        self.n = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.n += 1\n",
    "        return self.ft(self.start, self.end, self.n/self.num_it)\n",
    "    \n",
    "    def is_done(self):  return self.n >= self.num_it\n",
    "    def init_val(self): return self.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(Callback):\n",
    "    \n",
    "    def __init__(self, learn, lr_max, epochs, moms=(0.95,0.85), div_factor=10, pct_end=0.1):\n",
    "        self.learn = learn\n",
    "        a = int(len(learn.data.train_dl) * epochs * (1 - pct_end) / 2)\n",
    "        b = int(len(learn.data.train_dl) * epochs * pct_end)\n",
    "        self.lr_scheds = [Stepper((lr_max/div_factor, lr_max), a),\n",
    "                          Stepper((lr_max, lr_max/div_factor), a),\n",
    "                          Stepper((lr_max/div_factor, lr_max/(div_factor*100)), b)]\n",
    "        self.mom_scheds = [Stepper(moms, a), Stepper((moms[1], moms[0]), a), Stepper(moms[0], b)]\n",
    "    \n",
    "    def on_train_begin(self):\n",
    "        self.opt = self.learn.opt\n",
    "        self.opt.lr, self.opt.mom = self.lr_scheds[0].init_val(), self.mom_scheds[0].init_val()\n",
    "        self.idx_s = 0\n",
    "    \n",
    "    def on_batch_end(self, loss):\n",
    "        self.opt.lr = self.lr_scheds[self.idx_s].step()\n",
    "        self.opt.mom = self.mom_scheds[self.idx_s].step()\n",
    "        if self.lr_scheds[self.idx_s].is_done():\n",
    "            self.idx_s += 1\n",
    "            if self.idx_s >= len(self.lr_scheds): return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, data, model):\n",
    "        self.data,self.model = data,model.to(data.device)\n",
    "        self.loss_fn, self.opt_fn = F.cross_entropy, optim.SGD\n",
    "\n",
    "    def fit(self, epochs, lr, callbacks=[]):\n",
    "        self.opt = HPOptimizer(self.model.parameters(), self.opt_fn, init_lr=lr)\n",
    "        self.recorder = Recorder(self.opt, self.data.train_dl)\n",
    "        cbs = [self.recorder] + callbacks\n",
    "        fit(epochs, self.model, self.loss_fn, self.opt, self.data.train_dl, self.data.valid_dl, callbacks=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "learn = Learner(data, model)\n",
    "sched = OneCycleScheduler(learn, 0.1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(5,0.1,callbacks=[sched])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = list(range(len(learn.recorder.lrs)))\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,4))\n",
    "axs[0].plot(iterations, learn.recorder.lrs)\n",
    "axs[1].plot(iterations, learn.recorder.moms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder(Callback):\n",
    "    \n",
    "    def __init__(self, learn, start_lr=1e-5, end_lr=10, num_it=200):\n",
    "        self.learn = learn\n",
    "        self.sched = Stepper((start_lr, end_lr), num_it, annealing_exponential)\n",
    "    \n",
    "    def on_train_begin(self):\n",
    "        self.opt = self.learn.opt\n",
    "        self.opt.lr = self.sched.init_val()\n",
    "        self.stop,self.first,self.best_loss = False,True,0.\n",
    "    \n",
    "    def on_batch_end(self, loss):\n",
    "        if self.first or loss < self.best_loss:\n",
    "            self.first = False\n",
    "            self.best_loss = loss\n",
    "        self.opt.lr = self.sched.step()\n",
    "        if self.sched.is_done() or loss > 4*self.best_loss: \n",
    "            self.stop=True\n",
    "            return True\n",
    "    \n",
    "    def on_epoch_end(self, val_loss): return self.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, data, model):\n",
    "        self.data,self.model = data,model.to(data.device)\n",
    "        self.loss_fn, self.opt_fn = F.cross_entropy, optim.SGD\n",
    "\n",
    "    def fit(self, epochs, lr, callbacks=[], val=True):\n",
    "        self.opt = HPOptimizer(self.model.parameters(), self.opt_fn, init_lr=lr)\n",
    "        self.recorder = Recorder(self.opt, self.data.train_dl)\n",
    "        cbs = [self.recorder] + callbacks\n",
    "        fit(epochs, self.model, self.loss_fn, self.opt, self.data.train_dl, \n",
    "            self.data.valid_dl if val else None, callbacks=cbs)\n",
    "        \n",
    "    def lr_find(self, start_lr=1e-5, end_lr=10, num_it=200):\n",
    "        cb = LRFinder(self, start_lr, end_lr, num_it)\n",
    "        a = int(np.ceil(num_it/len(self.data.train_dl)))\n",
    "        self.fit(a, start_lr, callbacks=[cb], val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "learn = Learner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(learn.recorder.lrs, learn.recorder.losses)\n",
    "ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bit shaky since we plot the loss and not a smoothened version of it. Let's change the recorder so it records a moving average of the loss. We'll also add the plotting functions inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recorder(Callback):\n",
    "    beta = 0.98\n",
    "    \n",
    "    def __init__(self, opt, train_dl=None):\n",
    "        self.opt,self.train_dl = opt,train_dl\n",
    "    \n",
    "    def on_train_begin(self):\n",
    "        self.epoch,self.n,self.avg_loss = 0,0,0.\n",
    "        self.losses,self.val_losses,self.lrs,self.moms = [],[],[],[]\n",
    "    \n",
    "    def on_batch_begin(self, xb, yb):\n",
    "        self.lrs.append(self.opt.lr)\n",
    "        self.moms.append(self.opt.mom)\n",
    "        return xb, yb\n",
    "    \n",
    "    def on_backward_begin(self, loss):\n",
    "        #We record the loss here before any other callback has a chance to modify it.\n",
    "        self.n += 1\n",
    "        self.avg_loss = self.beta * self.avg_loss + (1-self.beta) * loss.item()\n",
    "        self.smooth_loss = self.avg_loss / (1 - self.beta ** self.n)\n",
    "        self.losses.append(self.smooth_loss)\n",
    "        if self.train_dl is not None and self.train_dl.progress_func is not None: \n",
    "            self.train_dl.gen.set_postfix_str(self.smooth_loss)\n",
    "    \n",
    "    def on_epoch_end(self, val_loss):\n",
    "        self.val_losses.append(val_loss)\n",
    "        print(self.epoch, val_loss)\n",
    "        self.epoch += 1\n",
    "    \n",
    "    def plot_lr(self, show_moms=False):\n",
    "        iterations = list(range(len(learn.recorder.lrs)))\n",
    "        if show_moms:\n",
    "            fig, axs = plt.subplots(1,2, figsize=(12,4))\n",
    "            axs[0].plot(iterations, self.lrs)\n",
    "            axs[1].plot(iterations, self.moms)\n",
    "        else: plt.plot(iterations, self.lrs)\n",
    "    \n",
    "    def plot(self, skip_start=10, skip_end=5):\n",
    "        lrs = self.lrs[skip_start:-skip_end] if skip_end > 0 else self.lrs[skip_start:]\n",
    "        losses = self.losses[skip_start:-skip_end] if skip_end > 0 else self.losses[skip_start:]\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        ax.plot(lrs, losses)\n",
    "        ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "learn = Learner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To grasp the potential of callbacks, here's a full example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyeOfSauron(Callback):\n",
    "    \n",
    "    def __init__(self, learn):\n",
    "        #By passing the learner, this callback will have access to everything:\n",
    "        #All the inputs/outputs as they go, the losses, but also the data loaders, the optimizer.\n",
    "        self.learn = learn\n",
    "        \n",
    "        #At any time:\n",
    "        #Changing self.learn.data.train_dl or self.data.valid_dl will change them inside the fit function\n",
    "        #(we just need to pass the data object to the fit function and not data.train_dl/data.valid_dl)\n",
    "        #Changing self.learn.opt.opt (We have an HPOptimizer on top of the actual optimizer) will change it \n",
    "        #inside the fit function.\n",
    "        #Changing self.learn.data or self.learn.opt directly WILL NOT change the data or the optimizer inside the fit function.\n",
    "\n",
    "    def on_train_begin(self):\n",
    "        #Here we can initiliaze anything we need. \n",
    "        self.opt = self.learn.opt\n",
    "        #The optimizer has now been initialized. We can change any hyper-parameters by typing\n",
    "        #self.opt.lr = new_lr, self.opt.mom = new_mom, self.opt.wd = new_wd or self.opt.beta = new_beta\n",
    "        \n",
    "    def on_epoch_begin(self):\n",
    "        #This is not technically useful since we have on_train_begin for epoch 0 and on_epoch_end for all the other epochs\n",
    "        #yet it makes writing code that needs to be done at the beginning of every epoch easy.\n",
    "        \n",
    "    def on_batch_begin(self, xb, yb):\n",
    "        #If we need to access anything inside the input or target for here or later, we can just save them.\n",
    "        self.xb,self.yb = xb,yb\n",
    "        #Here is the perfect place to prepare everything before the model is called.\n",
    "        #Example: change the values of the hyperparameters (if we don't do it on_batch_end instead)\n",
    "        \n",
    "        #If we return something, that will be the new value for xb,yb. \n",
    "    \n",
    "    def on_loss_begin(self, lout):\n",
    "        #If we need the output of the model for here or later, we can just save it.\n",
    "        self.out = out\n",
    "        #Here is the place to run some code that needs to be executed after the output has been computed but before the\n",
    "        #loss computation.\n",
    "        #Example: putting the output back in FP32 when training in mixed precision.\n",
    "        \n",
    "        #If we return something, that will be the new value for the output.\n",
    "    \n",
    "    def on_backward_begin(self, loss, out):\n",
    "        #If we need the loss of the model for here or later, we can just save it.\n",
    "        self.raw_loss,self.out = loss,out\n",
    "        #Here is the place to run some code that needs to be executed after the loss has been computed but before the\n",
    "        #gradient computation.\n",
    "        #Example: reg_fn in RNNs.\n",
    "        \n",
    "        #If we return something, that will be the new value for loss. Since the recorder is always called first,\n",
    "        #it will have the raw loss.\n",
    "        \n",
    "    def on_backward_end(self):\n",
    "        #Here is the place to run some code that needs to be executed after the gradients have been computed but\n",
    "        #before the optimizer is called.\n",
    "        #Example: deal with weight_decay in AdamW\n",
    "        \n",
    "    def on_step_end(self): \n",
    "        #Here is the place to run some code that needs to be executed after the optimizer step but before the gradients\n",
    "        #are zeroed\n",
    "        #Example: can't thnk of any but maybe someone will need this one day/\n",
    "        \n",
    "    def on_batch_end(self, loss):\n",
    "        #We get the loss again, this time it's the version modified by all the callbacks, so depending on our\n",
    "        #needs it might be best to use self.raw_loss\n",
    "        self.loss = loss\n",
    "        #Here is the place to run some code that needs to be executed after a batch is fully done.\n",
    "        #Example: change the values of the hyperparameters (if we don't do it on_batch_begin instead)\n",
    "        \n",
    "        #If we return true, the current epoch is interrupted (example: lr_finder stops the training when the loss explodes)\n",
    "        \n",
    "    def on_epoch_end(self, val_loss):\n",
    "        #We get the validation loss (TODO: and metrics)\n",
    "        self.val_loss = val_loss\n",
    "        #Here is the place to run some code that needs to be executed at the end of an epoch.\n",
    "        #Example: Save the model if we have a new best validation loss/metric.\n",
    "        \n",
    "        #If we return true, the training stops (example: early stopping)\n",
    "        \n",
    "    def on_train_end(self): \n",
    "        #Here is the place to tidy everything.\n",
    "        #Examples: save log_files, load best model found during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, xb, yb, loss_fn, opt=None, cb_handler=CallbackHandler([])):\n",
    "    out = model(xb)\n",
    "    out = cb_handler.on_loss_begin(out)\n",
    "    loss = loss_fn(out, yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss = cb_handler.on_backward_begin(loss)\n",
    "        loss.backward()\n",
    "        cb_handler.on_backward_end()\n",
    "        opt.step()\n",
    "        cb_handler.on_step_end()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, data, callbacks=[]):\n",
    "    \n",
    "    cb_handler = CallbackHandler(callbacks)\n",
    "    cb_handler.on_train_begin()\n",
    "    \n",
    "    for epoch in tnrange(epochs):\n",
    "        model.train()\n",
    "        cb_handler.on_epoch_begin()\n",
    "        \n",
    "        for xb,yb in data.train_dl:\n",
    "            xb, yb = cb_handler.on_batch_begin(xb, yb)\n",
    "            loss,_ = loss_batch(model, xb, yb, loss_fn, opt, cb_handler)\n",
    "            if cb_handler.on_batch_end(loss): break\n",
    "        \n",
    "        if hasattr(data,'valid_dl') and data.valid_dl is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                losses,nums = zip(*[loss_batch(model, xb, yb, loss_fn)\n",
    "                                for xb,yb in data.valid_dl])\n",
    "            val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "        else: val_loss=None\n",
    "        if cb_handler.on_epoch_end(val_loss): break\n",
    "        \n",
    "    cb_handler.on_train_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that one thing is entirely done in a callback so that it's easily read. For instance let's take back the LRFinder: on top of running the fit function with exponentially growing lrs, it needs to handle the fact some preparation and clean-up, and all this code should be in the callback, not the lr_find function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder(Callback):\n",
    "    #TODO: add model.save in init or on_train_begin and model.load in on_train_end.\n",
    "    \n",
    "    def __init__(self, learn, start_lr=1e-5, end_lr=10, num_it=200):\n",
    "        self.learn = learn\n",
    "        self.sched = Stepper((start_lr, end_lr), num_it, annealing_exponential)\n",
    "        #To avoid validating if the train_dl has less than num_it batches, we put aside the valid_dl and remove it\n",
    "        #during the call to fit.\n",
    "        self.valid_dl = learn.data.valid_dl\n",
    "        learn.data.valid_dl = None\n",
    "    \n",
    "    def on_train_begin(self):\n",
    "        self.opt = self.learn.opt\n",
    "        self.opt.lr = self.sched.init_val()\n",
    "        self.stop,self.first,self.best_loss = False,True,0.\n",
    "    \n",
    "    def on_batch_end(self, loss):\n",
    "        if self.first or loss < self.best_loss:\n",
    "            self.first = False\n",
    "            self.best_loss = loss\n",
    "        self.opt.lr = self.sched.step()\n",
    "        if self.sched.is_done() or self.learn.recorder.smooth_loss > 4*self.best_loss:\n",
    "            #We use the smoothed loss to decide on the stopping since it's less shaky.\n",
    "            self.stop=True\n",
    "            return True\n",
    "    \n",
    "    def on_epoch_end(self, val_loss): return self.stop\n",
    "    \n",
    "    def on_train_end(self):\n",
    "        #Clean up and put back the valid_dl in its place.\n",
    "        self.learn.data.valid_dl = self.valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, data, model):\n",
    "        self.data,self.model = data,model.to(data.device)\n",
    "        self.loss_fn, self.opt_fn = F.cross_entropy, optim.SGD\n",
    "\n",
    "    def fit(self, epochs, lr, callbacks=[]):\n",
    "        self.opt = HPOptimizer(self.model.parameters(), self.opt_fn, init_lr=lr)\n",
    "        self.recorder = Recorder(self.opt, self.data.train_dl)\n",
    "        callbacks.insert(0, self.recorder)\n",
    "        fit(epochs, self.model, self.loss_fn, self.opt, self.data, callbacks=callbacks)\n",
    "        \n",
    "    def lr_find(self, start_lr=1e-5, end_lr=10, num_it=200):\n",
    "        cb = LRFinder(self, start_lr, end_lr, num_it)\n",
    "        a = int(np.ceil(num_it/len(self.data.train_dl)))\n",
    "        self.fit(a, start_lr, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "learn = Learner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(learn.data.valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the tests for change of optimizers/dataloaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing directly opt.opt or data.train_dl/data.valid_dl changes the corresponding item in the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch.create(train_ds, valid_ds, bs=bs, train_tfm=tfms+[cifar_norm], valid_tfm=[cifar_norm], num_workers=0)\n",
    "data1 = DataBunch.create(train_ds, valid_ds, bs=32, train_tfm=tfms+[cifar_norm], valid_tfm=[cifar_norm], num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CbTest():\n",
    "    def __init__(self, learn, new_data):\n",
    "        self.learn,self.new_data = learn,new_data\n",
    "        \n",
    "    def call_me(self):\n",
    "        self.learn.data.train_dl = self.new_data.train_dl\n",
    "        self.learn.data.valid_dl = self.new_data.valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = CbTest(learn, data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data, cb):\n",
    "    x,y = next(iter(data.train_dl))\n",
    "    print(x.size())\n",
    "    cb.call_me()\n",
    "    x,y = next(iter(data.train_dl))\n",
    "    print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(learn.data, cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt = HPOptimizer(model.parameters(), optim.SGD, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CbTest():\n",
    "    def __init__(self, learn, new_opt):\n",
    "        self.learn,self.new_opt = learn,new_opt\n",
    "        \n",
    "    def call_me(self):\n",
    "        self.learn.opt.opt = self.new_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = CbTest(learn, optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(opt, cb):\n",
    "    print(opt.opt)\n",
    "    cb.call_me()\n",
    "    print(opt.opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(learn.opt,cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing directly opt or data doesn't change anything inside the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch.create(train_ds, valid_ds, bs=bs, train_tfm=tfms+[cifar_norm], valid_tfm=[cifar_norm], num_workers=0)\n",
    "data1 = DataBunch.create(train_ds, valid_ds, bs=32, train_tfm=tfms+[cifar_norm], valid_tfm=[cifar_norm], num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CbTest():\n",
    "    def __init__(self, learn, new_data):\n",
    "        self.learn,self.new_data = learn,new_data\n",
    "        \n",
    "    def call_me(self):\n",
    "        self.learn.data = self.new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = CbTest(learn, data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data, cb):\n",
    "    x,y = next(iter(data.train_dl))\n",
    "    print(x.size())\n",
    "    cb.call_me()\n",
    "    x,y = next(iter(data.train_dl))\n",
    "    print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(learn.data, cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt = optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CbTest():\n",
    "    def __init__(self, learn, new_opt):\n",
    "        self.learn,self.new_opt = learn,new_opt\n",
    "        \n",
    "    def call_me(self):\n",
    "        self.learn.opt = self.new_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = CbTest(learn, optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(opt, cb):\n",
    "    print(opt)\n",
    "    cb.call_me()\n",
    "    print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(learn.opt,cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Callable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class Learner():\n",
    "    \n",
    "    data: DataBunch\n",
    "    model: nn.Module\n",
    "    loss_fn: Callable = F.cross_entropy\n",
    "    opt_fn: Callable = optim.SGD\n",
    "    metrics: List = None\n",
    "    true_wd: bool = False\n",
    "    def __post_init__(self): self.model = self.model.to(self.data.device)\n",
    "\n",
    "    def fit(self, epochs, lr, wd=0., callbacks=None):\n",
    "        self.opt = HPOptimizer(self.model.parameters(), self.opt_fn, init_lr=lr, true_wd=self.true_wd)\n",
    "        self.opt.wd = wd\n",
    "        self.recorder = Recorder(self.opt, self.data.train_dl)\n",
    "        if callbacks is None: callbacks = []\n",
    "        callbacks.insert(0, self.recorder)\n",
    "        fit(epochs, self.model, self.loss_fn, self.opt, self.data, callbacks=callbacks, metrics=self.metrics)\n",
    "        \n",
    "    def lr_find(self, start_lr=1e-5, end_lr=10, num_it=200):\n",
    "        cb = LRFinder(self, start_lr, end_lr, num_it)\n",
    "        a = int(np.ceil(num_it/len(self.data.train_dl)))\n",
    "        self.fit(a, start_lr, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def loss_batch(model, xb, yb, loss_fn, opt=None, cb_handler=None):\n",
    "    if cb_handler is not None: xb, yb = cb_handler.on_batch_begin(xb, yb)\n",
    "    out = model(xb)\n",
    "    if cb_handler is not None: out = cb_handler.on_loss_begin(out)\n",
    "    loss = loss_fn(out, yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        if cb_handler is not None: loss = cb_handler.on_backward_begin(loss)\n",
    "        loss.backward()\n",
    "        if cb_handler is not None: cb_handler.on_backward_end()\n",
    "        opt.step()\n",
    "        if cb_handler is not None: cb_handler.on_step_end()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    if cb_handler is not None: stop = cb_handler.on_batch_end(loss.item()) else: False\n",
    "        \n",
    "    return loss.item(), len(xb), stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fit(epochs, model, loss_fn, opt, data, callbacks=None):\n",
    "    \n",
    "    cb_handler = CallbackHandler(callbacks)\n",
    "    cb_handler.on_train_begin()\n",
    "    \n",
    "    for epoch in tnrange(epochs):\n",
    "        model.train()\n",
    "        cb_handler.on_epoch_begin()\n",
    "        \n",
    "        for xb,yb in data.train_dl:\n",
    "            loss,_,stop = loss_batch(model, xb, yb, loss_fn, opt, cb_handler)\n",
    "            if stop: break\n",
    "        \n",
    "        if hasattr(data,'valid_dl') and data.valid_dl is not None:\n",
    "            model.eval()\n",
    "            cb_handler.on_valid_begin()\n",
    "            with torch.no_grad():\n",
    "                losses,nums = [],0\n",
    "                for xb, yb in data.valid_dl:\n",
    "                    loss,num,stop = loss_batch(model, xb, yb, loss_fn)\n",
    "                    losses.append(loss)\n",
    "                    nums.append(num)\n",
    "                    if stop: break\n",
    "            val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "        else: val_loss=None\n",
    "\n",
    "        if cb_handler.on_epoch_end(val_loss): break\n",
    "        \n",
    "    cb_handler.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Callback():\n",
    "    def on_train_begin(self, **kwargs): pass         \n",
    "        #To initiliaze constants in the callback.\n",
    "    def on_epoch_begin(self, **kwargs): pass\n",
    "        #At the beginning of each epoch\n",
    "    def on_batch_begin(self, **kwargs): pass \n",
    "        #To set HP before the step is done. A look at the input can be useful (set the lr depending on the seq_len in RNNs, \n",
    "        #or for reg_functions called in on_backward_begin)\n",
    "        #Returns xb, yb (which can allow us to modify the input at that step if needed)\n",
    "    def on_valid_begin(self, **kwars): pass\n",
    "        #If a valid_dl has been specified, then called at the start of validation\n",
    "    def on_loss_begin(self, **kwargs): pass\n",
    "        #Called after the forward pass but before the loss has been computed.\n",
    "        #Passes the output of the model.\n",
    "        #Returns the output (which can allow us to modify it)\n",
    "    def on_backward_begin(self, **kwargs): pass\n",
    "        #Called after the forward pass and the loss has been computed, but before the back propagation.\n",
    "        #Passes the loss of the model.\n",
    "        #Returns the loss (which can allow us to modify it, for instance for reg functions)\n",
    "    def on_backward_end(self, **kwargs): pass\n",
    "        #Called after the back propagation had been done (and the gradients computed) but before the step of the optimizer.\n",
    "        #Useful for true weight decay in AdamW\n",
    "    def on_step_end(self, **kwargs): pass\n",
    "        #Called after the step of the optimizer but before the gradients are zeroed (not sure this one is useful)\n",
    "    def on_batch_end(self, **kwargs): pass\n",
    "        #Called at the end of the batch\n",
    "    def on_epoch_end(self, **kwargs): pass\n",
    "        #Called at the end of an epoch\n",
    "    def on_train_end(self, **kwargs): pass\n",
    "        #Useful for cleaning up things and saving files/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricCallback(Callback):\n",
    "    def name(): pass\n",
    "        #provide a user friendly name for the metric for display purposes, i.e. Accuracy\n",
    "    def metric(): pass\n",
    "        #return the metric value, i.e. 0.9832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SmoothenValue():\n",
    "    \n",
    "    def __init__(self, beta):\n",
    "        self.beta,self.n,self.mov_avg = beta,0,0\n",
    "    \n",
    "    def add_value(self, val):\n",
    "        self.n += 1\n",
    "        self.mov_avg = self.beta * self.mov_avg + (1 - self.beta) * val\n",
    "        self.smooth = self.mov_avg / (1 - self.beta ** self.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CallbackHandler():\n",
    "    beta = 0.98\n",
    "    \n",
    "    def __init__(self, callbacks):\n",
    "        self.callbacks = callbacks if callbacks is not None else []\n",
    "        self.smoothener = SmoothenValue(self.beta)\n",
    "    \n",
    "    def __call__(self, cb_name):\n",
    "        return [getattr(cb, f'on_{cb_name}')(**self.state_dict) for cb in self.callbacks]\n",
    "    \n",
    "    def on_train_begin(self): \n",
    "        self.state_dict = {'epoch': 0, 'iteration': 0, 'num_batch': 0, 'batch_type': 'train'}\n",
    "        self('train_begin')\n",
    "        \n",
    "    def on_epoch_begin(self): \n",
    "        self.state_dict['num_batch'] = 0\n",
    "        self.state_dict['batch_type'] = 'train'\n",
    "        self('epoch_begin')\n",
    "        \n",
    "    def on_batch_begin(self, xb, yb):\n",
    "        self.state_dict['last_input'], self.state_dict['last_target'] = xb, yb\n",
    "        for cb in self.callbacks:\n",
    "            a = cb.on_batch_begin(**self.state_dict)\n",
    "            if a is not None: xb,yb = a\n",
    "        return xb,yb\n",
    "\n",
    "    def on_valid_begin(self, **kwars):\n",
    "        self.state_dict['batch_type'] = 'valid'\n",
    "    \n",
    "    def on_loss_begin(self, out):\n",
    "        self.state_dict['last_output'] = out\n",
    "        for cb in self.callbacks:\n",
    "            a = cb.on_loss_begin(**self.state_dict)\n",
    "            if a is not None: out = a\n",
    "        return out\n",
    "    \n",
    "    def on_backward_begin(self, loss):\n",
    "        self.smoothener.add_value(loss.item())\n",
    "        self.state_dict['last_loss'], self.state_dict['smooth_loss'] = loss, self.smoothener.smooth\n",
    "        for cb in self.callbacks:\n",
    "            a = cb.on_backward_begin(**self.state_dict)\n",
    "            if a is not None: loss = a\n",
    "        return loss\n",
    "    \n",
    "    def on_backward_end(self):        self('backward_end')\n",
    "    def on_step_end(self):            self('step_end')\n",
    "        \n",
    "    def on_batch_end(self, loss):     \n",
    "        stop = np.any(self('batch_end'))\n",
    "        self.state_dict['iteration'] += 1\n",
    "        self.state_dict['num_batch'] += 1\n",
    "        return stop\n",
    "    \n",
    "    def on_epoch_end(self, val_metrics):\n",
    "        self.state_dict['last_metrics'] = val_metrics\n",
    "        stop = np.any(self('epoch_end'))\n",
    "        self.state_dict['epoch'] += 1\n",
    "        return stop\n",
    "    \n",
    "    def on_train_end(self): self('train_end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy(MetricCallback):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.total,self.correct = 0,0\n",
    "        \n",
    "    def name():\n",
    "        return \"Accuracy\"\n",
    "    \n",
    "    def metric():\n",
    "        if self.total > 0: return self.correct / float(self.total)\n",
    "        else: return 0\n",
    "    \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.total,self.correct = 0,0\n",
    "\n",
    "    def on_loss_begin(self, batch_type, last_target, last_output):\n",
    "        if batch_type == 'valid':\n",
    "            preds = torch.max(last_output, dim=1)[1]\n",
    "            self.correct += torch.nonzero(preds == last_target).size(0)\n",
    "            self.total += len(last_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Recorder(Callback):\n",
    "    \n",
    "    def __init__(self, opt, train_dl=None):\n",
    "        self.opt,self.train_dl = opt,train_dl\n",
    "    \n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self.losses,self.val_losses,self.lrs,self.moms,self.metrics = [],[],[],[],[]\n",
    "    \n",
    "    def on_batch_begin(self, **kwargs):\n",
    "        self.lrs.append(self.opt.lr)\n",
    "        self.moms.append(self.opt.mom)\n",
    "    \n",
    "    def on_backward_begin(self, smooth_loss, **kwargs):\n",
    "        #We record the loss here before any other callback has a chance to modify it.\n",
    "        self.losses.append(smooth_loss)\n",
    "        if self.train_dl is not None and self.train_dl.progress_func is not None: \n",
    "            self.train_dl.gen.set_postfix_str(smooth_loss)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, last_metrics, **kwargs):\n",
    "        if last_metrics is not None:\n",
    "            self.val_losses.append(last_metrics[0])\n",
    "            if len(last_metrics) > 1: self.metrics.append(last_metrics[1:])\n",
    "            print(epoch, *last_metrics)\n",
    "    \n",
    "    def plot_lr(self, show_moms=False):\n",
    "        iterations = list(range(len(learn.recorder.lrs)))\n",
    "        if show_moms:\n",
    "            fig, axs = plt.subplots(1,2, figsize=(12,4))\n",
    "            axs[0].plot(iterations, self.lrs)\n",
    "            axs[1].plot(iterations, self.moms)\n",
    "        else: plt.plot(iterations, self.lrs)\n",
    "    \n",
    "    def plot(self, skip_start=10, skip_end=5):\n",
    "        lrs = self.lrs[skip_start:-skip_end] if skip_end > 0 else self.lrs[skip_start:]\n",
    "        losses = self.losses[skip_start:-skip_end] if skip_end > 0 else self.losses[skip_start:]\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        ax.plot(lrs, losses)\n",
    "        ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LRFinder(Callback):\n",
    "    #TODO: add model.save in init or on_train_begin and model.load in on_train_end.\n",
    "    \n",
    "    def __init__(self, learn, start_lr=1e-5, end_lr=10, num_it=200):\n",
    "        self.learn = learn\n",
    "        self.sched = Stepper((start_lr, end_lr), num_it, annealing_exponential)\n",
    "        #To avoid validating if the train_dl has less than num_it batches, we put aside the valid_dl and remove it\n",
    "        #during the call to fit.\n",
    "        self.valid_dl = learn.data.valid_dl\n",
    "        learn.data.valid_dl = None\n",
    "    \n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self.opt = self.learn.opt\n",
    "        self.opt.lr = self.sched.init_val()\n",
    "        self.stop,self.best_loss = False,0.\n",
    "    \n",
    "    def on_batch_end(self, iteration, smooth_loss, **kwargs):\n",
    "        if iteration==0 or smooth_loss < self.best_loss:\n",
    "            self.best_loss = smooth_loss\n",
    "        self.opt.lr = self.sched.step()\n",
    "        if self.sched.is_done() or smooth_loss > 4*self.best_loss:\n",
    "            #We use the smoothed loss to decide on the stopping since it's less shaky.\n",
    "            self.stop=True\n",
    "            return True\n",
    "    \n",
    "    def on_epoch_end(self, **kwargs): return self.stop\n",
    "    \n",
    "    def on_train_end(self, **kwargs):\n",
    "        #Clean up and put back the valid_dl in its place.\n",
    "        self.learn.data.valid_dl = self.valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OneCycleScheduler(Callback):\n",
    "    \n",
    "    def __init__(self, learn, lr_max, epochs, moms=(0.95,0.85), div_factor=10, pct_end=0.1):\n",
    "        self.learn = learn\n",
    "        a = int(len(learn.data.train_dl) * epochs * (1 - pct_end) / 2)\n",
    "        b = int(len(learn.data.train_dl) * epochs * pct_end)\n",
    "        self.lr_scheds = [Stepper((lr_max/div_factor, lr_max), a),\n",
    "                          Stepper((lr_max, lr_max/div_factor), a),\n",
    "                          Stepper((lr_max/div_factor, lr_max/(div_factor*100)), b)]\n",
    "        self.mom_scheds = [Stepper(moms, a), Stepper((moms[1], moms[0]), a), Stepper(moms[0], b)]\n",
    "    \n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self.opt = self.learn.opt\n",
    "        self.opt.lr, self.opt.mom = self.lr_scheds[0].init_val(), self.mom_scheds[0].init_val()\n",
    "        self.idx_s = 0\n",
    "    \n",
    "    def on_batch_end(self, **kwargs):\n",
    "        self.opt.lr = self.lr_scheds[self.idx_s].step()\n",
    "        self.opt.mom = self.mom_scheds[self.idx_s].step()\n",
    "        if self.lr_scheds[self.idx_s].is_done():\n",
    "            self.idx_s += 1\n",
    "            if self.idx_s >= len(self.lr_scheds): return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.max(out, dim=1)[1]\n",
    "    return (preds==yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "learn = Learner(data, model)\n",
    "learn.metrics = [accuracy]\n",
    "sched = OneCycleScheduler(learn, 0.1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(5, 1e-2, callbacks=[sched])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
